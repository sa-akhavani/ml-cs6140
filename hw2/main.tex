% CS6140 Homework Assignment Template
% Computer Science
% Northeastern University
% Boston, MA 02115

% Do not manipulate any of the settings
\documentclass[twoside]{article}

\usepackage{epsfig}
\usepackage{natbib}
\usepackage{units}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{babel}


\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[3]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CS6140: Machine Learning\hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Assigned: #2 \hfill Due: #3} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}

\begin{document}

% to have alphanumeric enumeration (Hasan's command)
\renewcommand{\labelenumi}{\alph{enumi})}

\lecture{Homework Assignment \# 2}{02/05/2020}{02/18/2020, 11:59pm, through Blackboard}

\begin{center}
Three problems, 95 points in total. Good luck!\\
Prof. Predrag Radivojac, Northeastern University
\end{center}

%%
%% Problem
%%

\textbf{Problem 1.} (25 points) Naive Bayes classifier. Consider a binary classification problem where there are only four data points in the training set. That is $\mathcal{D}=\left\{ (-1,-1,-),(-1,+1,+),(+1,-1,+),(+1,+1,-)\right\} $, where each tuple $(x_{1},x_{2},y)$ represents a training example with input vector $(x_{1},x_{2})$ and class label $y$.

\begin{enumerate}
\item (10 points) Construct a naive Bayes classifier for this problem and evaluate its accuracy on the training set. Consider ``accuracy'' to be the fraction of correct predictions.

\textbf{Solution:}

I: We should choose the max between these two:
\[
P(Y=+|(-1,-1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
\[
P(Y=-|(-1,-1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
So the result is: +

II: We should choose the max between these two:
\[
P(Y=+|(-1,+1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
\[
P(Y=-|(-1,+1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
So the result is: +

III: We should choose the max between these two:
\[
P(Y=+|(+1,-1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
\[
P(Y=-|(+1,-1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
So the result is: +

IV: We should choose the max between these two:
\[
P(Y=+|(+1,+1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
\[
P(Y=-|(+1,+1)) = 1/2 * 1/2 * 1/2 = 1/8
\]
So the result is: +

We will now calculate the accuracy. The formula had two correct results.
\[
accuracy  = 2 / 4 = 1/2
\]

\item (10 points) Transform the input space into a six-dimensional space $(+1, x_{1}, x_{2}, x_{1}x_{2}, x_{1}^{2}, x_{2}^{2})$ and repeat the previous step.

\textbf{Solution:}

So the new dataset is this:

$D = \{
(+1,-1,-1,+1,+1,+1,-),
(+1,-1,+1,-1,+1,+1,+),
$

$
(+1,+1,-1,-1,+1,+1,+),
(+1,+1,+1,+1,+1,+1,-)
\}
$

I: We should choose the max between these two:
\[
P(Y=+|(+1,-1,-1,+1,+1,+1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
\[
P(Y=-|(+1,-1,-1,+1,+1,+1)) = 2/2*1/2*1/2*2/2*2/2*2/2 *1/2 = 1/8
\]
So the result is: -

II: We should choose the max between these two:
\[
P(Y=+|(+1,-1,+1,-1,+1,+1)) = 2/2*1/2*1/2*2/2*2/2*2/2 *1/2 = 1/8
\]
\[
P(Y=-|(+1,-1,+1,-1,+1,+1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
So the result is: +

III: We should choose the max between these two:
\[
P(Y=+|(+1,+1,-1,-1,+1,+1)) = 2/2*1/2*1/2*2/2*2/2*2/2 *1/2 = 1/8
\]
\[
P(Y=-|(+1,+1,-1,-1,+1,+1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
So the result is: +

IV: We should choose the max between these two:
\[
P(Y=+|(+1,+1,+1,+1,+1,+1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
\[
P(Y=-|(+1,+1,+1,+1,+1,+1)) = 2/2*1/2*1/2*2/2*2/2*2/2 *1/2 = 1/8
\]
So the result is: -

We will now calculate the accuracy. The formula had four correct results.
\[
accuracy  = 4 / 4 = 1
\]

\item (5 points) Repeat the previous step when the data set accidentally includes the seventh feature, set to $-x_{1}x_{2}$. What is the impact of adding this dependent feature on the classification model?

\textbf{Solution:}

So the new dataset is this:

$D = \{
(+1,-1,-1,+1,+1,+1,-1,-),
(+1,-1,+1,-1,+1,+1,+1,+),
$

$
(+1,+1,-1,-1,+1,+1,+1,+),
(+1,+1,+1,+1,+1,+1,-1,-)
\}
$


I: We should choose the max between these two:
\[
P(Y=+|(+1,-1,-1,+1,+1,+1,-1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
\[
P(Y=-|(+1,-1,-1,+1,+1,+1,-1)) = 2/2*1/2*1/2*2/2*2/2*2/2 * 0 *1/2 = 0
\]
So the result is: +

II: We should choose the max between these two:
\[
P(Y=+|(+1,-1,+1,-1,+1,+1,+1)) = 2/2*1/2*1/2*2/2*2/2*2/2*0 *1/2 = 0
\]
\[
P(Y=-|(+1,-1,+1,-1,+1,+1,+1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
So the result is: +

III: We should choose the max between these two:
\[
P(Y=+|(+1,+1,-1,-1,+1,+1,+1)) = 2/2*1/2*1/2*2/2*2/2*2/2*0 *1/2 = 0
\]
\[
P(Y=-|(+1,+1,-1,-1,+1,+1,+1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
So the result is: +

IV: We should choose the max between these two:
\[
P(Y=+|(+1,+1,+1,+1,+1,+1,-1)) = 2/2 * 1/2 * 1/2 * 0/2 = 0
\]
\[
P(Y=-|(+1,+1,+1,+1,+1,+1,-1)) = 2/2*1/2*1/2*2/2*2/2*2/2*0 *1/2 = 0
\]
So the result is: +


We will now calculate the accuracy. The formula had two correct results. Adding this feature made our formula to generate wrong results for some inputs. It makes all the probabilities equal to zero so all of the predictions are now leading to + for the result.
\[
accuracy  = 2 / 4 = 1/2
\]

\end{enumerate}

%%
%% Problem
%%

\textbf{Problem 2.} (25 points) Consider a binary classification problem in which we want to determine the optimal decision surface. A point $\mathbf{x}$ is on the decision surface if $P(Y=1|\mathbf{x})=P(Y=0|\mathbf{x})$.

\begin{enumerate}
\item (10 points) Find the optimal decision surface assuming that each class-conditional distribution is defined as a two-dimensional Gaussian distribution:
\[
p(\mathbf{x}|Y=i)=\frac{1}{(2\pi)^{\nicefrac{d}{2}} |\mathbf{\Sigma}_{i}|^{\nicefrac{1}{2}}}\cdot e^{-\frac{1}{2}(\mathbf{x}-\mathbf{m}_{i})^{T}\mathbf{\Sigma}_{i}^{-1}(\mathbf{x}-\mathbf{m}_{i})}
\]
where $i \in \left\{ 0, 1\right\}$, $\mathbf{m}_{0}=(1,2)$, $\mathbf{m}_{1}=(6,3)$, $\mathbf{\Sigma}_{0}=\mathbf{\Sigma}_{1}=\mathbf{I}_2$, $P(Y=0)=P(Y=1)=\nicefrac{1}{2}$, $\mathbf{I}_d$ is the $d$-dimensional identity matrix, and $|\mathbf{\Sigma}_{i}|$ is the determinant of $\mathbf{\Sigma}_{i}$.
\item (5 points) Generalize the solution from part (a) using $\mathbf{m}_{0}=(m_{01}, m_{02})$, $\mathbf{m}_{1}=(m_{11}, m_{12})$, $\mathbf{\Sigma}_{0}=\mathbf{\Sigma}_{1}=\sigma^2 \mathbf{I}_2$ and $P(Y=0)\neq P(Y=1)$.
\item (10 points) Generalize the solution from part (b) to arbitrary covariance matrices $\mathbf{\Sigma}_{0}$ and $\mathbf{\Sigma}_{1}$. Discuss the shape of the optimal decision surface.
\end{enumerate}

%%
%% Problem
%%

\textbf{Problem 3.} (45 points) Consider a multivariate linear regression problem of mapping $\mathbb{R}^d$ to $\mathbb{R}$, with two different objective functions. The first objective function is the sum of squared errors, as presented in class; i.e., $\sum_{i=1}^{n}e_{i}^2$, where $e_i=w_0 + \sum_{j=1}^{d}w_jx_{ij} - y_i$. The second objective function is the sum of square Euclidean distances to the hyperplane; i.e., $\sum_{i=1}^{n}r_{i}^2$, where $r_i$ is the Euclidean distance between point $(x_i,y_i)$ to the hyperplane $f(x)=w_0 + \sum_{j=1}^{d}w_jx_j$.

\begin{enumerate}
\item (5 points) Derive a gradient descent algorithm to find the parameters of the model that minimizes the sum of squared errors. 

\item (20 points) Derive a gradient descent algorithm to find the parameters of the model that minimizes the sum of squared distances. 

\item (20 points) Implement both algorithms and test them on 5 datasets. Datasets can be randomly generated, as in class, or obtained from resources such as UCI Machine Learning Repository. Compare the solutions to the closed-form (maximum likelihood) solution derived in class and find the $R^2$ in all cases on the same dataset used to fit the parameters; i.e., do not implement cross-validation.
\end{enumerate}



\end{document}
